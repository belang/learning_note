############
Bibliography
############

Algorithm
=========

LASSO
=====


Regression shrinkage and selection via the lasso: a retrospective
-----------------------------------------------------------------

* Biblio: Tibshirani R.  Regression shrinkage and selection via the lasso J. R. Stat. Soc. Ser. B Stat. Methodol., 58 (1996), pp. 267-288
* Author: Robert Tibshirani. Stanford University, USA

What is LASSO
=============

* Biblio: https://www.mygreatlearning.com/blog/understanding-of-lasso-regression/

Deep learning in spiking neural networks
========================================


@article{TAVANAEI201947,
title = {Deep learning in spiking neural networks},
journal = {Neural Networks},
volume = {111},
pages = {47-63},
year = {2019},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
author = {Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timothée Masquelier and Anthony Maida},
keywords = {Deep learning, Spiking neural network, Biological plausibility, Machine learning, Power-efficient architecture},
abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.}

